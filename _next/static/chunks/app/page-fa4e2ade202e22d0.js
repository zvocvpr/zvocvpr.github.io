(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{1057:(e,a,t)=>{"use strict";t.r(a),t.d(a,{default:()=>X});var s=t(5155),i=t(6766),r=t(3453),n=t(4861),o=t(7434),l=t(3417),d=t(4213),c=t(9803),h=t(5040),m=t(3311),g=t(381),u=t(463),p=t(590),x=t(7520),b=t(3314),f=t(2111),v=t(257),y=t(5021),j=t(142),w=t(2713),N=t(2657),T=t(4867),A=t(7213),k=t(1243),O=t(7580),S=t(4869),V=t(1264),D=t(6516),E=t(9099),R=t(9037),I=t(3786),z=t(5604),L=t(6874),C=t.n(L);let M={headers:[{label:"Method",sub:null},{label:"KITTI",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"nuScenes",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"Argoverse",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"GTA",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]}],rows:[{segment:"Metric-Scale Zero-Shot Setting:"},{name:"XVO",kitti:["16.82","3.84","168.43","0.17"],nuscenes:["12.75","5.11","8.30","0.16"],argoverse:["9.13","4.86","5.70","0.12"],gta:["25.56","12.64","28.02","0.21"]},{name:"M+DS",kitti:["14.22","2.72","154.77","0.09"],nuscenes:["17.08",{val:"1.46",bold:!0},"10.46","0.18"],argoverse:["16.67",{val:"1.79",bold:!0},"8.51","0.13"],gta:["23.53","10.38","12.96","0.26"]},{name:"ZeroVO",kitti:["7.69","2.72","105.07","0.07"],nuscenes:["10.98","4.48","6.79","0.14"],argoverse:["6.83","3.13","4.10","0.11"],gta:["14.74","10.63","8.55",{val:"0.17",bold:!0}]},{name:"ZeroVO+",kitti:[{val:"6.81",bold:!0},{val:"2.69",bold:!0},{val:"104.69",bold:!0},{val:"0.06",bold:!0}],nuscenes:[{val:"9.74",bold:!0},"4.37",{val:"6.03",bold:!0},{val:"0.12",bold:!0}],argoverse:[{val:"4.64",bold:!0},"2.83",{val:"3.05",bold:!0},{val:"0.09",bold:!0}],gta:[{val:"13.42",bold:!0},{val:"7.99",bold:!0},{val:"8.24",bold:!0},{val:"0.17",bold:!0}]},{name:"LiteZeroVO+",kitti:["8.85","2.90","118.54","0.08"],nuscenes:["11.57","4.44","6.87","0.13"],argoverse:["7.65","3.82","5.28","0.11"],gta:["15.93","12.16","11.26","0.18"]},{segment:"Baselines Requiring Ground-Truth Scale Alignment:"},{name:"TartanVO",kitti:["13.85","3.27","103.07","-"],nuscenes:["10.27","6.35","6.26","-"],argoverse:["11.17","5.30","7.03","-"],gta:["10.56","9.35","3.82","-"]},{name:"DPVO",kitti:["8.31","2.37","78.53","-"],nuscenes:["4.34","2.85","2.66","-"],argoverse:["2.66","1.25","1.59","-"],gta:["12.65","10.67","4.33","-"]}],caption:"Table 1: Comparative Analysis Across Datasets.We compare ZeroVO variants with existing baselines using standard metrics of translation, rotation, absolute trajectory, and scale errors. All methods are provided with estimated camera intrinsics and metric depth. ZeroVO+ is our model trained with further data using semi-supervision, and LiteZeroVO+ is a smaller model variant for resource-constrained settings. Our models demonstrate strong performance across metrics and datasets, particularly in metric translation estimation. As highlighted by the scale error, GTA and nuScenes contain challenging evaluation settings, including nighttime, weather variations, haze, and reflections. We note that TartanVO and DPVO baselines (in gray) only predict up-to-scale motion and use privileged information, ie, ground-truth scale alignment in evaluation."},_={headers:["Method","Day","Night","Rainy","Strong Light"],rows:[{name:"XVO [37]",day:"6.61",night:"14.41",rainy:"15.99",light:"15.73"},{name:"M+DS [27]",day:"6.08",night:"17.19",rainy:"17.49",light:"18.54"},{name:"ZeroVO",day:"3.90",night:"10.33",rainy:"12.63",light:"13.33"},{name:"ZeroVO+",day:"3.60",night:"10.26",rainy:"10.10",light:"11.15"}],caption:"Table 2: Condition Breakdown on nuScenes (ATE). We show results breakdown (ATE) over scenes categorized by weather and lens settings. We sample from nuScenes the Day, Night, and Rainy scenes, along with particularly challenging frames that include severe light reflections. Our ZeroVO+ model performs best overall. We note that TartanVO and DPVO baselines only predict up-to-scale motion and use ground-truth scale alignment in inference."},Z={headers:["Language","Geometry","KITTI (ATE)","nuScenes (ATE)","Argoverse (ATE)","GTA (ATE)"],rows:[{lang:"✓",geo:"✗",kitti:"119.15",nuscenes:"7.98",argoverse:"4.88",gta:"21.73"},{lang:"✓",geo:"✓",kitti:"103.23",nuscenes:"7.46",argoverse:"3.65",gta:"18.99"}],caption:"Table 3: Pseudo-Label Selection. Language and geometry-based selection are complementary for semi-supervised training"},F={headers:["RCR","IHF","COR","KITTI (ATE)","nuScenes (ATE)","Argoverse (ATE)","GTA (ATE)"],rows:[{rcr:"✓",ihf:"✗",cor:"✗",kitti:"109.55",nuscenes:"8.13",argoverse:"4.02",gta:"24.91"},{rcr:"✓",ihf:"✓",cor:"✗",kitti:"103.23",nuscenes:"7.46",argoverse:"3.65",gta:"18.99"},{rcr:"✓",ihf:"✓",cor:"✓",kitti:"113.62",nuscenes:"7.88",argoverse:"4.16",gta:"18.53"}],caption:"Table 4: Impact of Data Augmentation. We find both RCR (Random Crop and Resize) and IHF (Image Horizontal Flip) to benefit model generalization. However, leveraging image corruption and stylized augmentations (denoted as COR) provides marginal benefits."},G={headers:[{label:"Method / Noise",sub:null},{label:"0%",sub:["t_err","r_err","ATE","s_err"]},{label:"10%",sub:["t_err","r_err","ATE","s_err"]},{label:"20%",sub:["t_err","r_err","ATE","s_err"]},{label:"30%",sub:["t_err","r_err","ATE","s_err"]}],rows:[{segment:"Metric-Scale Zero-Shot Setting"},{name:"M+DS",kitti:["14.22","2.72","154.77","0.09"],nuscenes:["15.65","2.90","161.58","0.09"],argoverse:["17.55","3.74","179.92","0.10"],gta:["23.33","4.45","259.74","0.11"]},{name:"ZeroVO+",kitti:[{val:"7.23",bold:!0},{val:"2.55",bold:!0},{val:"103.23",bold:!0},{val:"0.06",bold:!0}],nuscenes:[{val:"7.44",bold:!0},{val:"2.71",bold:!0},{val:"117.10",bold:!0},{val:"0.06",bold:!0}],argoverse:[{val:"11.33",bold:!0},{val:"3.27",bold:!0},{val:"153.95",bold:!0},{val:"0.08",bold:!0}],gta:[{val:"13.75",bold:!0},{val:"3.85",bold:!0},{val:"163.87",bold:!0},{val:"0.10",bold:!0}]},{segment:"Baselines Requiring Ground-Truth Scale Alignment"},{name:"TartanVO",kitti:["13.85","3.27","103.07","-"],nuscenes:["13.87","3.39","109.78","-"],argoverse:["16.90","3.83","130.09","-"],gta:["18.55","4.11","133.24","-"]},{name:"DPVO",kitti:["8.31","2.37","78.53","-"],nuscenes:["15.32","2.79","152.56","-"],argoverse:["23.54","3.15","196.46","-"],gta:["36.92","4.90","280.49","-"]}],caption:"Table 5: Impact of Noisy Intrinsics Parameters. We inject Gaussian noise into the estimated camera intrinsic parameters to analyze the impact on DROID-SLAM and ZeroVO. We note that scale error cannot be computed for up-to-scale methods."},P={headers:[{label:"",sub:["F"]},{label:"",sub:["D"]},{label:"",sub:["L"]},{label:"",sub:["S"]},{label:"",sub:["P"]},{label:"KITTI",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"nuScenes",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"Argoverse",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]},{label:"GTA",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]}],rows:[{name:"✓",d:"",l:"",s:"",p:"",kitti:["18.76","5.49","174.24","0.18"],nuscenes:["19.40","7.42","12.54","0.22"],argoverse:["12.23","6.34","9.42","0.20"],gta:["25.68","15.52","25.38","0.25"]},{name:"✓",d:"✓",l:"",s:"",p:"",kitti:["8.99","2.92","123.42","0.08"],nuscenes:["12.26","5.23","8.40","0.15"],argoverse:["8.62","4.11","5.71","0.11"],gta:["16.76","12.75","12.37","0.19"]},{name:"✓",d:"✓",l:"✓",s:"",p:"",kitti:["7.69","2.72","105.07","0.07"],nuscenes:["10.98","4.48","6.79","0.14"],argoverse:["6.83","3.13","4.10","0.11"],gta:["14.74","10.63","8.55","0.17"]},{name:"✓",d:"✓",l:"✓",s:"✓",p:"",kitti:["9.11","2.88","117.49","0.08"],nuscenes:["12.25","5.39","7.53","0.14"],argoverse:["7.98","3.95","5.13","0.11"],gta:["16.49","11.95","10.27","0.18"]},{name:"✓",d:"✓",l:"✓",s:"✓",p:"✓",kitti:[{val:"6.81",bold:!0},{val:"2.69",bold:!0},{val:"104.69",bold:!0},{val:"0.06",bold:!0}],nuscenes:[{val:"9.74",bold:!0},{val:"4.37",bold:!0},{val:"6.03",bold:!0},{val:"0.12",bold:!0}],argoverse:[{val:"4.64",bold:!0},{val:"2.83",bold:!0},{val:"3.05",bold:!0},{val:"0.09",bold:!0}],gta:[{val:"13.42",bold:!0},{val:"7.99",bold:!0},{val:"8.24",bold:!0},{val:"0.17",bold:!0}]}],caption:"Table 7: Ablation Analysis for Model and Training Components. We analyze various model components: Flow module (F), Depth module (D), Language prior (L), Semi-supervised training (S), and Pseudo-label Selection (P). Flow, depth, and language correspond to the proposed supervised ZeroVO model. Results with additional semi-supervised training are shown as ZeroVO+ (showing state-of-the-art performance by integrating all of our proposed components"},B={headers:[{label:"",sub:["Sequence Length"]},{label:"Metrics",sub:["tₑᵣᵣ","rₑᵣᵣ","ATE","sₑᵣᵣ"]}],rows:[{name:"Entire Sequence",metrics:["14.22","2.72","154.77","0.09"]},{name:"50 Frames",metrics:["14.40","2.85","158.23","0.09"]},{name:"10 Frames",metrics:["16.31","3.51","180.67","0.10"]},{name:"2 Frames",metrics:["75.24","40.78","409.95","0.94"]}],caption:"Table 6: DROID-SLAM Ablation. DROID-SLAM leverages multi-frame input and optimization (in contrast, ZeroVO only relies on a two-frame input). We find significant degradation in performance when reducing the number of input frames into DROID-SLAM (with Metric3DV2)."},W=e=>{let{icon:a,title:t,id:i}=e;return(0,s.jsxs)("div",{id:i,className:"flex items-center space-x-3 mb-6 pt-16 -mt-16",children:[" ",(0,s.jsx)(a,{className:"w-8 h-8 text-primary"}),(0,s.jsx)("h2",{className:"text-3xl font-bold tracking-tight",children:t})]})},H=e=>{let{icon:a,title:t,children:i}=e;return(0,s.jsxs)("div",{className:"bg-gray-50 p-6 rounded-lg shadow",children:[(0,s.jsxs)("div",{className:"flex items-center space-x-3 mb-3",children:[(0,s.jsx)(a,{className:"w-6 h-6 text-secondary"}),(0,s.jsx)("h3",{className:"text-xl font-semibold",children:t})]}),(0,s.jsx)("div",{className:"text-gray-700 space-y-2",children:i})]})},q=e=>{let{data:a,fullWidth:t=!1}=e;return(0,s.jsxs)("div",{className:"overflow-x-auto ".concat(t?"w-full":"max-w-4xl mx-auto"," my-6"),children:[(0,s.jsxs)("table",{className:"min-w-full divide-y divide-gray-200 border border-gray-200 rounded-lg shadow-sm",children:[(0,s.jsx)("thead",{className:"bg-gray-50",children:(0,s.jsx)("tr",{children:a.headers.map((e,a)=>(0,s.jsx)("th",{scope:"col",className:"px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider",children:e},a))})}),(0,s.jsx)("tbody",{className:"bg-white divide-y divide-gray-200",children:a.rows.map((e,a)=>(0,s.jsx)("tr",{className:a%2==0?"bg-white":"bg-gray-50",children:Object.values(e).map((e,a)=>(0,s.jsx)("td",{className:"px-4 py-3 whitespace-nowrap text-sm text-center ".concat("object"==typeof e&&(null==e?void 0:e.bold)?"font-bold text-black":"text-gray-700"),children:"✓"===e||"✓ (rare)"===e?(0,s.jsx)(r.A,{className:"text-green-500 mx-auto"}):"✗"===e?(0,s.jsx)(n.A,{className:"text-red-500 mx-auto"}):"object"==typeof e?e.val:e},a))},a))})]}),a.caption&&(0,s.jsx)("p",{className:"mt-2 text-sm text-gray-600 text-center",children:a.caption})]})},K=e=>{let{data:a,fullWidth:t=!1}=e;return(0,s.jsxs)("div",{className:"overflow-x-auto ".concat(t?"w-full":"max-w-5xl mx-auto"," my-6"),children:[(0,s.jsxs)("table",{className:"min-w-full border border-gray-300 rounded-md shadow-sm text-sm",children:[(0,s.jsxs)("thead",{children:[(0,s.jsx)("tr",{className:"bg-gray-100",children:a.headers.map((e,a)=>(0,s.jsx)("th",{colSpan:e.sub?e.sub.length:1,className:"px-4 py-2 border-b border-r font-semibold text-gray-700 text-center",children:e.label},a))}),(0,s.jsx)("tr",{className:"bg-gray-50",children:a.headers.map((e,a)=>e.sub?e.sub.map((e,t)=>(0,s.jsx)("th",{className:"px-4 py-2 border-b border-r font-medium text-gray-600 text-center",children:e},"".concat(a,"-").concat(t))):(0,s.jsx)("th",{className:"px-4 py-2 border-b border-r font-medium text-gray-600 text-center"},"".concat(a,"-only")))})]}),(0,s.jsx)("tbody",{children:a.rows.map((e,t)=>{if(e.segment)return(0,s.jsx)("tr",{className:"bg-gray-200",children:(0,s.jsx)("td",{colSpan:a.headers.reduce((e,a)=>e+(a.sub?a.sub.length:1),0),className:"px-4 py-2 font-bold text-gray-700 text-left border-t border-b border-gray-400",children:e.segment})},"segment-".concat(t));let i=Object.entries(e).filter(e=>{let[a]=e;return"name"!==a}).flatMap(e=>{let[a,t]=e;return t});return(0,s.jsxs)("tr",{className:t%2==0?"bg-white":"bg-gray-50",children:[(0,s.jsx)("td",{className:"px-4 py-2 border-r text-center font-medium text-gray-800",children:e.name}),i.map((e,a)=>(0,s.jsx)("td",{className:"px-4 py-2 border border-gray-300 text-center text-sm text-gray-700",children:".."===e||"... (rare)"===e?(0,s.jsx)(r.A,{className:"text-green-500 mx-auto"}):"✗"===e?(0,s.jsx)(n.A,{className:"text-red-500 mx-auto"}):"object"==typeof e&&e.bold?(0,s.jsx)("span",{className:"font-bold text-black",children:e.val}):e},a))]},t)})})]}),a.caption&&(0,s.jsx)("p",{className:"mt-2 text-sm text-gray-500 text-center",children:a.caption})]})},Y=e=>{let{src:a,alt:t,caption:r}=e;return(0,s.jsxs)("div",{className:"text-center",children:[(0,s.jsx)("div",{className:"relative w-full aspect-[4/3] rounded-lg overflow-hidden shadow-lg border border-gray-200",children:(0,s.jsx)(i.default,{src:a||"/placeholder.svg",alt:t,layout:"fill",objectFit:"contain",className:"bg-gray-100"})}),r&&(0,s.jsx)("p",{className:"mt-2 text-sm text-gray-600 italic",children:r})]})};function X(){let e="@inproceedings{donotciteplaceholder,\n  title={{ZeroVO}: Visual Odometry with Minimal Assumptions},\n  author={Lai, Lei and Yin, Zekai and Ohn-Bar, Eshed},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2025}\n}";return(0,s.jsxs)("div",{className:"min-h-screen bg-gray-100 text-gray-800",children:[(0,s.jsx)("header",{className:"bg-gradient-to-r from-primary to-secondary py-12 text-white shadow-lg",children:(0,s.jsxs)("div",{className:"container mx-auto px-4 text-center",children:[(0,s.jsx)("h1",{className:"text-5xl font-extrabold mb-3 tracking-tight",children:"ZeroVO: Visual Odometry with Minimal Assumptions"}),(0,s.jsxs)("div",{className:"flex flex-wrap justify-center items-center gap-8 text-white text-xl md:text-2xl lg:text-3xl font-semibold mb-10",children:[(0,s.jsx)("a",{href:"https://leilai125.github.io/",target:"_blank",rel:"noopener noreferrer",className:"hover:underline",children:"Lei Lai*"}),(0,s.jsx)("a",{href:"https://zekai-yin.github.io",target:"_blank",rel:"noopener noreferrer",className:"hover:underline",children:"Zekai Yin*"}),(0,s.jsx)("a",{href:"https://eshed1.github.io",target:"_blank",rel:"noopener noreferrer",className:"hover:underline",children:"Eshed Ohn-Bar"})]}),(0,s.jsx)("p",{className:"text-xl mb-6 max-w-3xl mx-auto",children:"A novel VO algorithm achieving zero-shot generalization across diverse cameras and environments, without predefined camera calibration."}),(0,s.jsxs)("div",{className:"flex justify-center space-x-4 mb-8",children:[(0,s.jsx)(C(),{href:"/assets/CAMERA_READY.pdf",passHref:!0,legacyBehavior:!0,children:(0,s.jsxs)("a",{className:"bg-white text-primary font-semibold py-2 px-6 rounded-lg shadow-md hover:bg-gray-200 transition duration-150 flex items-center space-x-2",children:[(0,s.jsx)(o.A,{size:20})," ",(0,s.jsx)("span",{children:"Paper (PDF)"})]})}),(0,s.jsx)(C(),{href:"#code-link",passHref:!0,legacyBehavior:!0,children:(0,s.jsxs)("a",{className:"bg-white text-primary font-semibold py-2 px-6 rounded-lg shadow-md hover:bg-gray-200 transition duration-150 flex items-center space-x-2",children:[(0,s.jsx)(l.A,{size:20})," ",(0,s.jsx)("span",{children:"Code (GitHub)"})]})}),(0,s.jsx)(C(),{href:"#gta-dataset-link",passHref:!0,legacyBehavior:!0,children:(0,s.jsxs)("a",{className:"bg-white text-primary font-semibold py-2 px-6 rounded-lg shadow-md hover:bg-gray-200 transition duration-150 flex items-center space-x-2",children:[(0,s.jsx)(d.A,{size:20})," ",(0,s.jsx)("span",{children:"GTA Dataset"})]})}),(0,s.jsx)(C(),{href:"#supplementary-video-link",passHref:!0,legacyBehavior:!0,children:(0,s.jsxs)("a",{className:"bg-white text-primary font-semibold py-2 px-6 rounded-lg shadow-md hover:bg-gray-200 transition duration-150 flex items-center space-x-2",children:[(0,s.jsx)(c.A,{size:20})," ",(0,s.jsx)("span",{children:"Supplementary Video"})]})})]})]})}),(0,s.jsx)("nav",{className:"sticky top-0 bg-white shadow-md z-50",children:(0,s.jsx)("div",{className:"container mx-auto px-4",children:(0,s.jsx)("div",{className:"flex justify-center space-x-6 py-3 overflow-x-auto",children:["Abstract","Innovations","Methodology","GTA Dataset","Experiments","Ablations","Qualitative","Resources"].map(e=>(0,s.jsx)(C(),{href:"#".concat(e.toLowerCase().replace(" ","-")),passHref:!0,legacyBehavior:!0,children:(0,s.jsx)("a",{className:"text-gray-600 hover:text-primary font-medium transition duration-150 whitespace-nowrap",children:e})},e))})})}),(0,s.jsxs)("main",{className:"container mx-auto px-4 py-12",children:[(0,s.jsxs)("section",{id:"video-demo",className:"mb-16 bg-white p-8 rounded-lg shadow-xl",children:[(0,s.jsx)(W,{icon:c.A,title:"Video Demonstration",id:"video-demo-content"}),(0,s.jsx)("div",{className:"aspect-video relative rounded-lg overflow-hidden shadow-lg border border-gray-200",children:(0,s.jsx)("video",{src:"/tmep/assets/example.mp4",controls:!0,autoPlay:!0,muted:!0,loop:!0,playsInline:!0,poster:"/assets/pipeline.png?height=720&width=1280",className:"absolute inset-0 w-full h-full object-cover bg-black",children:"Your browser does not support the video tag."})}),(0,s.jsx)("p",{className:"text-center mt-4 text-gray-600",children:"Watch ZeroVO in action, demonstrating its robustness across various scenarios."})]}),(0,s.jsxs)("section",{id:"abstract",className:"mb-16 bg-white p-8 rounded-lg shadow-xl",children:[(0,s.jsx)(W,{icon:h.A,title:"Abstract",id:"abstract-content"}),(0,s.jsx)("p",{className:"text-lg leading-relaxed text-gray-700",children:"We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves zero-shot generalization across diverse cameras and environments, overcoming limitations in existing methods that depend on predefined or static camera calibration setups. Our approach incorporates three main innovations. First, we design a calibration-free, geometry-aware network structure capable of handling noise in estimated depth and camera parameters. Second, we introduce a language-based prior that infuses semantic information to enhance robust feature extraction and generalization to previously unseen domains. Third, we develop a flexible, semi-supervised training paradigm that iteratively adapts to new scenes using unlabeled data, further boosting the models’ ability to generalize across diverse real-world scenarios. We analyze complex autonomous driving contexts, demonstrating over 30% improvement against prior methods on three standard benchmarks—KITTI, nuScenes, and Argoverse 2—as well as a newly introduced, high-fidelity synthetic dataset derived from Grand Theft Auto (GTA). By not requiring fine-tuning or camera calibration, our work broadens the applicability of VO, providing a versatile solution for real-world deployment at scale."})]}),(0,s.jsxs)("section",{id:"innovations",className:"mb-16",children:[(0,s.jsx)(W,{icon:m.A,title:"Key Innovations",id:"innovations-content"}),(0,s.jsxs)("div",{className:"grid md:grid-cols-3 gap-8",children:[(0,s.jsx)(H,{icon:g.A,title:"Calibration-Free Geometry-Aware Network",children:(0,s.jsx)("p",{children:"Designed to handle noise in estimated depth and camera parameters without requiring predefined calibration."})}),(0,s.jsx)(H,{icon:u.A,title:"Language-Based Semantic Prior",children:(0,s.jsx)("p",{children:"Infuses semantic information via language models to enhance robust feature extraction and generalization to unseen domains."})}),(0,s.jsx)(H,{icon:p.A,title:"Flexible Semi-Supervised Training",children:(0,s.jsx)("p",{children:"Iteratively adapts to new scenes using unlabeled data, boosting generalization across diverse real-world scenarios."})})]})]}),(0,s.jsxs)("section",{id:"methodology",className:"mb-16",children:[(0,s.jsx)(W,{icon:x.A,title:"Methodology",id:"methodology-content"}),(0,s.jsxs)("div",{className:"bg-white p-8 rounded-lg shadow-xl space-y-8",children:[(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(b.A,{className:"mr-2 text-secondary"}),"Network Architecture"]}),(0,s.jsx)("p",{className:"text-gray-700 mb-4",children:"ZeroVO employs a Transformer-based architecture to fuse multimodal priors while reasoning over structure and noisy pseudo-3D information. Key components include:"}),(0,s.jsxs)("ul",{className:"list-disc list-inside space-y-2 pl-4 text-gray-700",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Intrinsic Parameter Estimation:"})," Leverages WildCamera for single-image intrinsic parameter estimation, avoiding reliance on fixed camera settings."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Multimodal Image Cues:"})," Extracts optical flow (MaskFlownet), metric depth (Metric3Dv2), and language-based context (LLaVA-NeXT & Sentence Transformers)."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Unprojection to Pseudo-3D:"})," Estimated depth is unprojected to a 3D point cloud, and 2D optical flow to 3D scene flow, guided by estimated intrinsics."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Language and Geometry-Guided Transformer:"})," Fuses multimodal cues (depth, flow, intrinsics, language) using cross-attention mechanisms to produce robust features."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Probabilistic Ego-Motion Decoder:"})," An MLP with two branches decodes features into metric-scale translation and rotation (modeled by a matrix Fisher distribution)."]})]}),(0,s.jsxs)("div",{className:"my-6 p-4 border-l-4 border-primary bg-blue-50 rounded-r-md",children:[(0,s.jsx)("p",{className:"font-semibold text-primary",children:"Architecture (Fig. 1 from paper)"}),(0,s.jsx)("div",{className:"mt-2 relative w-full overflow-auto flex justify-center",children:(0,s.jsx)(i.default,{src:"/assets/pipeline.png",alt:"ZeroVO Architecture Diagram",width:3050,height:1360,className:"bg-gray-100 rounded shadow"})}),(0,s.jsx)("p",{className:"text-sm text-gray-600 mt-2 text-center",children:"Multimodal and Geometry-Guided Network Overview. Given a pair of input images, our model computes a rich multimodal embedding through a transformer-based fusion module. The embedding is then passed to a two-branch decoder MLP that outputs real-world translation and rotation. Our architecture (Sec. 3.1) leverages cross-attention to fuse complementary cues, including flow, depth, camera intrinsics, and language-based features in a geometry-aware manner. The language prior is first used to refine both the depth map and 2D flow estimates. The refined depth is then unprojected into 3D (using estimated parameters) to compute scene flow, which is further enhanced and fused with additional features before decoding. By embedding geometric reasoning and multimodal priors directly into the network structure, our model achieves strong zero-shot generalization across diverse and challenging settings."})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(f.A,{className:"mr-2 text-secondary"}),"Model Training"]}),(0,s.jsx)("p",{className:"text-gray-700 mb-4",children:"ZeroVO is trained using Adam optimizer for 100 epochs. The initial teacher model is trained on nuScenes-OneNorth."}),(0,s.jsxs)("ul",{className:"list-disc list-inside space-y-2 pl-4 text-gray-700",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Supervised Training:"})," Optimized using MSE loss for translation and negative log-likelihood for rotation, without requiring privileged ground-truth for flow, depth, or camera parameters."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Semi-Supervised Training (ZeroVO+):"})," Refined using pseudo-labels from an unconstrained YouTube dataset. This involves:",(0,s.jsxs)("ul",{className:"list-circle list-inside space-y-1 pl-6 mt-1",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Geometry-Guided Pseudo-Label Selection:"})," Filters noisy samples based on warping consistency (normSSIM ","<"," 0.5)."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Language-Guided Pseudo-Label Selection:"})," Filters redundant samples based on text feature similarity over a temporal window (subspace-sim ","<"," 5.0)."]})]})]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Data Augmentations:"})," Random Crop and Resize (RCR) to simulate diverse camera intrinsics and Image Horizontal Flip (IHF) are employed. COR (visual corruptions) showed marginal benefits."]})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(v.A,{className:"mr-2 text-secondary"}),"Model Variants"]}),(0,s.jsxs)("ul",{className:"list-disc list-inside space-y-2 pl-4 text-gray-700",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"ZeroVO:"})," Default supervised model."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"ZeroVO+:"})," Further trained with semi-supervision and multimodal pseudo-label selection. Inference: ~0.6 FPS (constrained by LLaVA-NeXT)."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"LiteZeroVO+:"})," Resource-constrained variant omitting language-conditioned input modules (uses self-attention instead of cross-attention for language). Inference: ~5 FPS."]})]})]})]})]}),(0,s.jsxs)("section",{id:"experiments",className:"mb-16",children:[(0,s.jsx)(W,{icon:y.A,title:"Experimental Setup & Results",id:"experiments-content"}),(0,s.jsxs)("div",{className:"bg-white p-8 rounded-lg shadow-xl space-y-8",children:[(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(j.A,{className:"mr-2 text-secondary"}),"Evaluation Metrics"]}),(0,s.jsxs)("p",{className:"text-gray-700 mb-4",children:["We evaluate using standard VO metrics: Translation Error (t",(0,s.jsx)("sub",{children:"err"})," %), Rotation Error (r",(0,s.jsx)("sub",{children:"err"})," \xb0/100m), Absolute Trajectory Error (ATE), and Scale Error (s",(0,s.jsx)("sub",{children:"err"}),"). Unlike methods evaluating up-to-scale, we focus on metric-scale translation."]})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(w.A,{className:"mr-2 text-secondary"}),"Quantitative Results"]}),(0,s.jsx)("p",{className:"text-gray-700 mb-4",children:"ZeroVO variants are compared against state-of-the-art baselines (XVO, M+DS, TartanVO, DPVO) on KITTI, nuScenes, Argoverse 2, and our GTA dataset. All methods are provided with estimated camera intrinsics and metric depth for fair zero-shot comparison."}),(0,s.jsx)(K,{data:M,fullWidth:!0}),(0,s.jsxs)("div",{className:"my-6 p-4 border-l-4 border-primary bg-blue-50 rounded-r-md",children:[(0,s.jsx)("p",{className:"font-semibold text-primary",children:"Architecture Diagram (Fig. 1 from paper)"}),(0,s.jsx)("div",{className:"mt-2 relative w-full overflow-auto flex justify-center",children:(0,s.jsx)(i.default,{src:"/assets/KITTI.png",alt:"ZeroVO Architecture Diagram",width:2617,height:641,className:"bg-gray-100 rounded shadow"})}),(0,s.jsx)("p",{className:"text-sm text-gray-600 mt-2 text-center",children:"Qualitative Results on KITTI. We show trajectory prediction results across the four most complex driving sequences (00, 02, 05, and 08) from the KITTI dataset. Each subplot illustrates the trajectories generated by our proposed model and the baseline models alongside the ground truth trajectory. The qualitative results demonstrate that our approach achieves the highest alignment with the ground truth, particularly in challenging turns and extended straight paths. These findings highlight the robustness of our method in handling complex and diverse driving scenarios."})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(N.A,{className:"mr-2 text-secondary"}),"Performance under Challenging Conditions"]}),(0,s.jsx)("p",{className:"text-gray-700 mb-4",children:"We analyze performance on nuScenes subsets based on weather conditions (Day, Night, Rain) and challenging light reflections."}),(0,s.jsx)(q,{data:_})]}),(0,s.jsxs)("div",{children:[(0,s.jsxs)("h3",{className:"text-2xl font-semibold mb-3 flex items-center",children:[(0,s.jsx)(N.A,{className:"mr-2 text-secondary"}),"Ablation Studies"]}),(0,s.jsx)("p",{children:"The main paper's ablation study (Table 2) demonstrates the impact of each core module (Flow, Depth, Language prior) and the benefits of semi-supervised training with pseudo-label selection (S+P). Each component contributes to the overall performance, with the full ZeroVO+ model achieving the best results."}),(0,s.jsx)(K,{data:P,fullWidth:!0})]})]})]}),(0,s.jsxs)("section",{id:"ablations",className:"mb-16",children:[(0,s.jsx)(W,{icon:T.A,title:"More Experiments",id:"ablations-content"}),(0,s.jsxs)("div",{className:"bg-white p-8 rounded-lg shadow-xl space-y-10",children:[(0,s.jsxs)(H,{icon:j.A,title:"Pseudo-Label Selection (Table 2 Supp.)",children:[(0,s.jsx)("p",{children:"Both language-guided (for diversity) and geometry-guided (for noise exclusion) selection mechanisms are complementary and crucial for optimal semi-supervised training performance."}),(0,s.jsx)(q,{data:Z})]}),(0,s.jsxs)(H,{icon:A.A,title:"Data Augmentation Strategies",children:[(0,s.jsx)("p",{children:"Random Crop and Resize (RCR) significantly impacts rotation estimation by simulating diverse camera intrinsics. Image Horizontal Flip (IHF) also benefits generalization. Synthetic weather/stylization (COR) showed marginal improvements."}),(0,s.jsx)(q,{data:F})]}),(0,s.jsxs)(H,{icon:k.A,title:"Impact of Noisy Intrinsic Parameters",children:[(0,s.jsx)("p",{children:"Increased noise in estimated camera intrinsics significantly degrades performance for all models. ZeroVO+ demonstrates greater robustness compared to baselines under highly noisy intrinsic settings."}),(0,s.jsx)(K,{data:G,fullWidth:!0})]}),(0,s.jsxs)(H,{icon:x.A,title:"DROID-SLAM Ablation",children:[(0,s.jsx)("p",{children:"DROID-SLAM's performance (M+DS) declines significantly as the input sequence length decreases, highlighting its dependence on multi-frame optimization. ZeroVO, a two-frame method, is not affected by this."}),(0,s.jsx)(K,{data:B})]})]})]}),(0,s.jsxs)("section",{id:"qualitative",className:"mb-16",children:[(0,s.jsx)(W,{icon:A.A,title:"GTA Dataset Examples",id:"qualitative-content"}),(0,s.jsx)("div",{className:"bg-white p-8 rounded-lg shadow-xl space-y-10",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-2xl font-semibold mb-4",children:"GTA Dataset Examples"}),(0,s.jsx)("p",{className:"text-gray-700 mb-6",children:"Qualitative examples from our GTA dataset showcasing diverse off-road (desert, forest, mountain) and on-road (city, highway, night) scenarios under various weather conditions, with LLaVA-generated captions."}),(0,s.jsxs)("div",{className:"grid md:grid-cols-2 lg:grid-cols-3 gap-6",children:[(0,s.jsx)(Y,{src:"/assets/desert_807_000.png?height=300&width=400",alt:"GTA Off-Road Desert",caption:" The image depicts a driving scenario where a vehicle is traveling on a dirt road that cuts through a landscape dominated by wind turbines. The road appears to be unpaved and dusty, suggesting it might be a rural or remote area . The wind turbines are tall and white, with multiple blades that are likely designed to capture wind energy. The sky is clear and blue, indicating good weather conditions. The terrain is hilly with sparse vegetation, and the overall setting suggests a location that is likely used for renewable energy production."}),(0,s.jsx)(Y,{src:"/assets/forest_57_000.png?height=300&width=400",alt:"GTA Off-Road Forest Fog",caption:"The image depicts a driving scenario in a forested area . The road appears to be unpaved and is surrounded by tall trees, suggesting a rural or remote location . There is a layer of fog or mist that is rising from the ground, which could indicate recent rain or damp conditions. The lighting suggests it might be either early morning or late afternoon, as the sky is not brightly lit, and the shadows are long. The overall atmosphere is serene and somewhat mysterious due to the fog and the quiet, natural setting. "}),(0,s.jsx)(Y,{src:"/assets/mountain_14_000.png?height=300&width=400",alt:"GTA Off-Road Mountain Cliff",caption:"The image depicts a driving scenario on a narrow, unpaved road that appears to be winding along a cliff edge. The road is not well-maintained , with visible tire tracks and loose gravel, suggesting that it is frequently used by vehicles. The cliff on the left side of the road is steep and rocky, with sparse vegetation growing along the edge. The sky is overcast, and the lighting suggests it could be either early morning or late afternoon. The overall atmosphere is one of a rugged, possibly remote location, and the road seems to be a challenge for drivers."}),(0,s.jsx)(Y,{src:"/assets/sunny_city_778_000.png?height=300&width=400",alt:"GTA On-Road City Traffic Light",caption:"In the image, we see a driving scenario where a black convertible car is stopped at a traffic light. The car is positioned on the right side of the road, as indicated by the traffic light and the road markings. The traffic light is displaying a red signal , indicating that the car must stop. The car’s brake lights are on , and the driver appears to be waiting for the light to change before proceeding. There are buildings on both sides of the street, and the architecture suggests a mix of commercial and residential structures. The road is marked with a crosswalk, and there are traffic signs visible, including one that reads 'DO NOT BLOCK INTERSECTION,' which is likely there to prevent vehicles from stopping in a way that could obstruct traffic flow.overall scene is typical of an urban environment where traffic rules are enforced, and drivers are expected to follow the signals and signs to ensure the smooth flow of traffic and pedestrian safety."}),(0,s.jsx)(Y,{src:"/assets/rain_highway.png?height=300&width=400",alt:"GTA On-Road Highway Rain",caption:"The image depicts a driving scenario on a highway during what appears to be a rainy day. The road is wet, reflecting the light, and there are multiple lanes with vehicles traveling in both directions. The vehicles include a black car in the foreground, which is the main focus of the image, and other cars in the background. The highway is bordered by a barrier on the right side, and there are trees and a hillside on the left side, suggesting a coastal or hilly region. Above the highway , there is a bridge or overpass with a green sign that is not clearly legible. The sky is overcast, and the lighting suggests it could be either dawn or dusk. The overall atmosphere of the image is somewhat moody and atmospheric, with a focus on the motion of the vehicles and the wet road conditions."}),(0,s.jsx)(Y,{src:"/assets/rain_night_highway.png?height=300&width=400",alt:"GTA On-Road Night Wet Road",caption:"The image depicts a nighttime driving scenario on a road that appears to be wet , possibly due to recent rain. The visibility is reduced, and the atmosphere is dark, with the streetlights casting a glow on the wet surface. There are several vehicles on the road, including a blue and yellow taxi in the foreground, a white bus in the background, and other cars in between. The vehicles are moving along the road, and the traffic seems to be flowing , albeit at a slower pace due to the wet conditions. The road is bordered by a barrier on the left side, and there are palm trees lining the right side, suggesting a location that could be in a region with a warm climate. The overall scene is typical of a city street during a rainy evening, with the challenge of navigating through the reduced visibility and wet road conditions."})]}),(0,s.jsx)("p",{className:"text-center mt-4 text-gray-600",children:"These examples illustrate the diverse scenarios in our GTA dataset, showcasing ZeroVO's robustness across challenging conditions."})]})})]}),(0,s.jsxs)("section",{id:"team",className:"mb-16",children:[(0,s.jsx)(W,{icon:O.A,title:"Research Team",id:"team-content"}),(0,s.jsx)("div",{className:"grid md:grid-cols-3 gap-10",children:[{name:"Lei Lai*",image:"/assets/leilai.jpg",homepage:"https://leilai125.github.io/",affiliation:"Boston University",email:"leilai@bu.edu"},{name:"Zekai Yin*",image:"/assets/zekaiyin_.png",homepage:"https://zekai-yin.github.io",affiliation:"Boston University",email:"zekaiyin@bu.edu"},{name:"Eshed Ohn-Bar",image:"/assets/eshedohnbar.jpg",homepage:"https://eshed1.github.io",affiliation:"Boston University",email:"eohnbar@bu.edu"}].map(e=>(0,s.jsxs)("div",{className:"bg-white p-6 rounded-lg shadow-xl text-center",children:[(0,s.jsxs)("a",{href:e.homepage||"mailto:".concat(e.email),target:e.homepage?"_blank":"_self",rel:e.homepage?"noopener noreferrer":"",className:"block group",children:[(0,s.jsx)("div",{className:"relative w-36 h-36 mx-auto mb-4 rounded-full overflow-hidden border-4 border-gray-200 group-hover:border-primary transition-colors",children:(0,s.jsx)(i.default,{src:e.image||"/placeholder.svg?height=144&width=144&query=".concat(e.name," portrait"),alt:e.name,width:144,height:144,className:"object-cover transition-transform group-hover:scale-105"})}),(0,s.jsx)("h3",{className:"text-xl font-semibold group-hover:text-primary transition-colors",children:e.name})]}),(0,s.jsx)("p",{className:"text-gray-600 text-sm",children:e.affiliation}),(0,s.jsxs)("div",{className:"mt-3 flex justify-center space-x-3",children:[e.homepage&&(0,s.jsx)("a",{href:e.homepage,target:"_blank",rel:"noopener noreferrer",title:"Homepage",className:"text-gray-500 hover:text-primary",children:(0,s.jsx)(S.A,{size:20})}),e.email&&(0,s.jsx)("a",{href:"mailto:".concat(e.email),title:"Email",className:"text-gray-500 hover:text-primary",children:(0,s.jsx)(V.A,{size:20})})]})]},e.name))})]}),(0,s.jsxs)("section",{id:"resources",className:"mb-16",children:[(0,s.jsx)(W,{icon:D.A,title:"Resources & Citation",id:"resources-content"}),(0,s.jsxs)("div",{className:"bg-white p-8 rounded-lg shadow-xl grid md:grid-cols-2 gap-8",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-2xl font-semibold mb-4",children:"Downloads & Links"}),(0,s.jsx)("ul",{className:"space-y-3",children:[{href:"/assets/CAMERA_READY.pdf",text:"Paper (PDF)",icon:o.A},{href:"#code-link",text:"Code (GitHub)",icon:E.A},{href:"#gta-dataset-link",text:"GTA Dataset",icon:d.A},{href:"#supplementary-video-link",text:"Supplementary Video",icon:c.A},{href:"#cvpr-link",text:"CVPR 2025",icon:R.A}].map(e=>(0,s.jsx)("li",{children:(0,s.jsx)(C(),{href:e.href,passHref:!0,legacyBehavior:!0,children:(0,s.jsxs)("a",{target:"_blank",rel:"noopener noreferrer",className:"flex items-center text-primary hover:underline font-medium",children:[(0,s.jsx)(e.icon,{size:20,className:"mr-2"})," ",e.text," ",(0,s.jsx)(I.A,{size:16,className:"ml-1 opacity-70"})]})})},e.text))})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-2xl font-semibold mb-4",children:"Cite This Work"}),(0,s.jsxs)("div",{className:"bg-gray-50 p-4 rounded-md border border-gray-200 relative",children:[(0,s.jsx)("pre",{className:"text-sm whitespace-pre-wrap break-all font-mono text-gray-700",children:e}),(0,s.jsx)("button",{onClick:()=>{navigator.clipboard.writeText(e).then(()=>alert("BibTeX copied to clipboard!")).catch(e=>console.error("Failed to copy BibTeX: ",e))},className:"absolute top-2 right-2 bg-gray-200 hover:bg-gray-300 text-gray-700 p-1.5 rounded",title:"Copy BibTeX to clipboard",children:(0,s.jsx)(z.A,{size:18})})]})]})]})]})]}),(0,s.jsx)("footer",{className:"bg-gray-800 text-gray-300 py-8 text-center",children:(0,s.jsxs)("div",{className:"container mx-auto px-4",children:[(0,s.jsxs)("p",{children:["\xa9 ",new Date().getFullYear()," Lei Lai, Zekai Yin, Eshed Ohn-Bar. All rights reserved."]}),(0,s.jsx)("p",{className:"text-sm mt-1",children:"ZeroVO presented at CVPR 2025 (Paper ID 7666)."}),(0,s.jsx)("p",{className:"text-xs mt-2",children:"Website template generated with assistance from v0."})]})})]})}},9930:(e,a,t)=>{Promise.resolve().then(t.bind(t,1057))}},e=>{var a=a=>e(e.s=a);e.O(0,[874,460,441,684,358],()=>a(9930)),_N_E=e.O()}]);