<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="ZeroVO: Visual Odometry with Minimal Assumptions">
  <meta name="keywords" content="ZeroVO: Visual Odometry with Minimal Assumptions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZeroVO: Visual Odometry with Minimal Assumptions</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="shortcut icon" href="./website_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:site_name" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta property="og:description" content="Lai, et al. ZeroVO: Visual Odometry with Minimal Assumptions." />

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta name="twitter:description" content="Lai, et al. ZeroVO: Visual Odometry with Minimal Assumptions." />

  <script src="https://www.youtube.com/iframe_api"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">ZeroVO: Visual Odometry with Minimal Assumptions</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://leilai125.github.io/">Lei Lai*</a>&emsp;
                <a href="https://zekai-yin.github.io">Zekai Yin*</a>&emsp;
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>&emsp;
                <br />Boston University
                <span class="brmod"></span>CVPR 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="./resources/CAMERA_READY.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.08005" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="#method_video" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/h2xlab/ZeroVO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://pan.baidu.com/s/1Hj9F4fC7JlDhG-QOpuGjzA?pwd=adth#list/path=%2F" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Datasets</span>
                  </a>
                </span>
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
                  We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves zero-shot generalization across diverse cameras and environments, overcoming limitations in existing methods that depend on predefined or static camera calibration setups. Our approach incorporates three main innovations. First, we design a calibration-free, geometry-aware network structure capable of handling noise in estimated depth and camera parameters. Second, we introduce a language-based prior that infuses semantic information to enhance robust feature extraction and generalization to previously unseen domains. Third, we develop a flexible, semi-supervised training paradigm that iteratively adapts to new scenes using unlabeled data, further boosting the models' ability to generalize across diverse real-world scenarios. We analyze complex autonomous driving contexts, demonstrating over 30% improvement against prior methods on three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly introduced, high-fidelity synthetic dataset derived from Grand Theft Auto (GTA). By not requiring fine-tuning or camera calibration, our work broadens the applicability of VO, providing a versatile solution for real-world deployment at scale.
            </p>
          </div>
        </div>
      </div>
    </div>

  </section>

  <section class="section">
    <div class="container">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            Our method facilitates generalization via minimal and versatile image-based priors, integrated throughout our model structure. Given a pair of input images, our model computes a rich multimodal embedding through a transformer-based fusion module. The embedding is then passed to a two-branch decoder MLP that outputs realworld translation and rotation. Our architecture leverages cross-attention to fuse complementary cues, including flow, depth,
            camera intrinsics, and language-based features in a geometry-aware manner. The language prior is first used to refine both the depth map
            and 2D flow estimates. The refined depth is then unprojected into 3D (using estimated parameters) to compute scene flow, which is further
            enhanced and fused with additional features before decoding. By embedding geometric reasoning and multimodal priors directly into the
            network structure, our model achieves strong zero-shot generalization across diverse and challenging settings.
            </p>
            
            </div>
            <div class="column">
              <img src="./resources/pipeline.png" width="100%" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Analysis</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Comparative Analysis Across Datasets.</b> 
            We compare ZeroVO variants with existing baselines using standard metrics of translation, rotation, absolute trajectory, and scale errors. All methods are provided with estimated camera intrinsics and metric depth. ZeroVO+ is
            our model trained with further data using semi-supervision, and LiteZeroVO+ is a smaller model variant for resource-constrained settings.
            Our models demonstrate strong performance across metrics and datasets, particularly in metric translation estimation. As highlighted by
            the scale error, GTA and nuScenes contain challenging evaluation settings, including nighttime, weather variations, haze, and reflections.
            We note that TartanVO and DPVO baselines (in gray) only predict up-to-scale motion and use privileged information, i.e., ground-truth
            scale alignment in evaluation.
            </p>
          </div>
          <div class="column">
            <img src="./resources/cross_datasets.png" width="100%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section mt-0 pt-0">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Ablation Analysis for Model and Training Components.</b> 
            We analyze various model components: Flow module (<b>F</b>), Depth
            module (<b>D</b>), Language prior (<b>L</b>), Semi-supervised training (<b>S</b>), and Pseudo-label Selection (<b>P</b>). Flow, depth, and language correspond to
            the proposed supervised ZeroVO model. Results with additional semi-supervised training are shown as ZeroVO+ (showing state-of-the-art
            performance by integrating all of our proposed components).
            </p>
          </div>
          <div class="column">
            <img src="./resources/ablation.png" width="100%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section mt-0 pt-0">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Qualitative Results on KITTI.</b> 
            We show trajectory prediction results across the four most complex driving sequences (00, 02,
            05, and 08) from the KITTI dataset. Each subplot illustrates the trajectories generated by our proposed model and the baseline models
            alongside the ground truth trajectory. The qualitative results demonstrate that our approach achieves the highest alignment with the ground
            truth, particularly in challenging turns and extended straight paths. These findings highlight the robustness of our method in handling
            complex and diverse driving scenarios.
            </p>
          </div>
          <div class="column">
            <img src="./resources/traj_vis.png" width="100%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Examples</h2>
            <video controls>
            <source src="./resources/examples.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>


  <!-- <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">GTA V</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            We introduce a newly generated simulated dataset derived from the high-fidelity, GTA simulation. Our GTA dataset consists of 922 driving sequences captured within a simulated city environment, encompassing a range of diverse weather
            conditions, driving speeds (particularly high-speed maneuvers not found in other public datasets), traffic scenarios, and times of day.  Compared to other commonly used opensource simulation platforms such as CARLA, GTA offers several key advantages: (1) enhanced image realism
            through the application of the reshade graphic settings that support higher quality rendering, and (2) a wider variety of road conditions across various weather scenarios. For onroad driving, these conditions include significant uphill and downhill gradients, tunnels, and underground parking facilities; for off-road driving, the environment features mountains, deserts, snow-covered terrains, and forests, thereby enabling more precise and complex rotational dynamics throughout the map.
            </p>
          </div>
          <div class="column">
            <img src="./resources/desert_807_000.png" width="50%" />
          </div>
          <div class="column">
            <img src="./resources/forest_57_000.png" width="50%" />
          </div>
          <div class="column">
            <img src="./resources/mountain_14_000.png" width="50%" />
          </div>
          <div class="column">
            <img src="./resources/sunny_city_778_000.png" width="50%" />
          </div>
          <div class="column">
            <img src="./resources/rain_highway.png" width="50%" />
          </div>
          <div class="column">
            <img src="./resources/rain_night_highway.png" width="50%" />
          </div>
        </div>
      </div>
    </div>
  </section> -->

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">GTA V</h2>
        <div class="content has-text-justified">
          <p class="my-paragraph">
            We introduce a newly generated simulated dataset derived from the high-fidelity GTA simulation. Our GTA dataset consists of 922 driving sequences captured within a simulated city environment, encompassing a range of diverse weather conditions, driving speeds (particularly high-speed maneuvers not found in other public datasets), traffic scenarios, and times of day. Compared to other commonly used open-source simulation platforms such as CARLA, GTA offers several key advantages: (1) enhanced image realism through the application of reshade graphic settings that support higher quality rendering, and (2) a wider variety of road conditions across various weather scenarios. For on-road driving, these conditions include significant uphill and downhill gradients, tunnels, and underground parking facilities; for off-road driving, the environment features mountains, deserts, snow-covered terrains, and forests, thereby enabling more precise and complex rotational dynamics throughout the map.
          </p>
        </div>

        <!-- Row 1 -->
        <div class="columns is-mobile">
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Off-Road Desert</p>
            <img src="./resources/desert_807_000.png" width="100%" />
          </div>
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Foggy Forest Trail</p>
            <img src="./resources/forest_57_000.png" width="100%" />
          </div>
        </div>

        <!-- Row 2 -->
        <div class="columns is-mobile">
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Mountain Cliff Path</p>
            <img src="./resources/mountain_14_000.png" width="100%" />
          </div>
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Urban Intersection (Sunny)</p>
            <img src="./resources/sunny_city_778_000.png" width="100%" />
          </div>
        </div>

        <!-- Row 3 -->
        <div class="columns is-mobile">
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Highway in Rain</p>
            <img src="./resources/rain_highway.png" width="100%" />
          </div>
          <div class="column is-half has-text-centered">
            <p class="subtitle is-6">Nighttime Highway (Rain)</p>
            <img src="./resources/rain_night_highway.png" width="100%" />
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

</section>




  <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Acknowledgments</h2>
          <div class="content has-text-centered">
            We thank the Red Hat Collaboratory (awards 2024-01-
            RH02, 2024-01-RH07) and National Science Foundation
            (IIS-2152077) for supporting this research.
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">BibTeX</h2>
          <div class="content has-text-justified">
            <pre><code>@inproceedings{lai2025zerovo,
              title={ZeroVO: Visual Odometry with Minimal Assumptions},
              author={Lai, Lei and Yin, Zekai and Ohn-Bar, Eshed},
              booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
              pages={17092--17102},
              year={2025}
            }</code></pre>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a href="./resources/CAMERA_READY.pdf" class="large-font bottom_buttons">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a href="https://github.com/h2xlab/ZVO" class="large-font bottom_buttons">
          <i class="fab fa-github"></i>
        </a>
        <br />
        <p>Page template borrowed from <a href="https://https://catdrive.github.io/"><span class="dnerf">CaT</span></a>
      </div>
    </div>
  </footer>

</body>

</html>
