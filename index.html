<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="XVO: Generalized Visual Odometry via Cross-Modal Self-Training">
  <meta name="keywords" content="XVO: Generalized Visual Odometry">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>XVO: Generalized Visual Odometry via Cross-Modal Self-Training</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="shortcut icon" href="./xvo.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:site_name" content="XVO: Generalized Visual Odometry via Cross-Modal Self-Training" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="XVO: Generalized Visual Odometry via Cross-Modal Self-Training" />
  <meta property="og:description" content="Lai, et al. XVO: Generalized Visual Odometry via Cross-Modal Self-Training." />

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="XVO: Generalized Visual Odometry via Cross-Modal Self-Training" />
  <meta name="twitter:description" content="Lai, et al. XVO: Generalized Visual Odometry via Cross-Modal Self-Training." />

  <script src="https://www.youtube.com/iframe_api"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">XVO: Generalized Visual Odometry via Cross-Modal Self-Training</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://cs-people.bu.edu/leilai/">Lei Lai*</a>&emsp;
                <a href="https://www.linkedin.com/in/zhongkai-leon-shangguan-52b465152">Zhongkai Shangguan*</a>&emsp;
                <a href="https://jimuyangz.github.io/">Jimuyang Zhang</a>&emsp;
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>&emsp;
                <br />Boston University
                <span class="brmod"></span>ICCV 2023</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="./resources/XVO.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.16772" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="#method_video" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/h2xlab/XVO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-6">
        <img src="./resources/example1.gif" />
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
    </div>
  </div>
</section>
 -->
  
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> </h2>
          <div class="content has-text-justified">
            <p>
            </p>
            <div class="column">
              <img src="./resources/generalized_vo.png" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
                  We propose XVO, a semi-supervised learning method for training generalized 
              monocular Visual Odometry (VO) models with robust off-the-self operation 
              across diverse datasets and settings. In contrast to standard monocular 
              VO approaches which often study a known calibration within a single dataset, 
              XVO efficiently learns to recover relative pose with real-world scale from 
              visual scene semantics, i.e., without relying on any known camera parameters. 
              We optimize the motion estimation model via self-training from large amounts 
              of unconstrained and heterogeneous dash camera videos available on YouTube. 
              Our key contribution is twofold. First, we empirically demonstrate the benefits 
              of semi-supervised training for learning a general-purpose direct VO regression 
              network. Second, we demonstrate multi-modal supervision, including segmentation, 
              flow, depth, and audio auxiliary prediction tasks, to facilitate generalized 
              representations for the VO task. Specifically, we find audio prediction task 
              to significantly enhance the semi-supervised learning process while alleviating 
              noisy pseudo-labels, particularly in highly dynamic and out-of-domain video data. 
              Our proposed teacher network achieves state-of-the-art performance on the commonly 
              used KITTI benchmark despite no multi-frame optimization or knowledge of camera 
              parameters. Combined with the proposed semi-supervised step, XVO demonstrates 
              off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, 
              and Argoverse without fine-tuning. 
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <!-- <br />
    <br />
    <div id="method_video" class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video"> -->
          <!-- <video controls>
            <source src="./resources/CaT_Video.mp4" type="video/mp4">
          </video> -->
          <!-- <iframe src="https://www.youtube.com/embed/5tmkDHfgqvU"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

  </section>




  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Motivation</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
              Our approach is inspired by how humans learn general representations 
              through observation of large amouts of multi-modal data, 
              and the successes witnessed in computer vision 
              through multi-task frameworks and auxiliary learning.

              <br>
              
              Humans can flexibly estimate motion in arbitrary conditions through 
              a general understanding of salient scene properties (e.g., object sizes).
              This general understanding is developed over large amounts of perceptual 
              data, often multi-modal in nature in arbitrary conditions through a general 
              understanding of salient scene properties.
              For instance, cross-modal information processing between audio and video 
              has been shown to play a role in spatial reasoning and proprioception.
              Drawing from this human learning paradigm, our approach leverages extensive 
              multimodal data through a semi-supervised training process, incorporating 
              various auxiliary prediction tasks, including segmentation, flow, depth, 
              and audio, to achieve generalized visual odometry without any camera intrinsic 
              or extrinsic information in diverse environment under arbitrary conditions.
              Our approach is also motivated by the successes witnessed in computer vision 
              through multi-task frameworks, where auxiliary tasks, in addition to the 
              primary learning objective, serve as a regulation to reduce the overfitting 
              and enrich the feature representations.

              <br>
            </p>
            <div class="column">
              <img src="./resources/motivation.png" />
            </div>
             <p class="my-paragraph">             
              The figure above illustrates the importance of audio in estimating vehicle acceleration and braking.
              The frame is consistent with the red arrow marked on the waveform. On the left, the audio amplitude 
              decreases and maintains a low level when the vehicle is going to wait for traffic lights.
              On the right, the audio experiences many ups and downs associated with acceleration and braking in a narrow urban area.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            Our proposed framework comprises the following steps: 
            (1) uncertainty-aware training of an initial (i.e., teacher) VO model; 
            (2) pseudo-labeling with the removal of low-confidence and potentially noisy samples; 
            (3) self-training with pseudo-labeled and auxiliary prediction tasks of a robust VO student model. 
            Specifically, we first train an ego-motion prediction teacher model over a small initial dataset, e.g., nuScenes.
            We then expand the original dataset through pseudo-labeling of in-the-wild videos.
            After that, we employ multiple auxiliary prediction tasks, including segmentation, flow, depth, and audio, as part of the semi-supervised training process
            Finally, we leverage uncertainty-based filtering of potentially noisy pseudo labels and train a robust student model.
            </p>
            <!--
            <ol>
              <li>We first train an ego-motion prediction teacher model over a small initial dataset, e.g., nuScenes</li>

              <li>We then expand the original dataset through pseudo-labeling of in-the-wild videos.</li>

              <li>we employ multiple auxiliary prediction tasks, including segmentation, flow, depth, and audio, as part of the semi-supervised training process</li>

              <li>We leverage uncertainty-based filtering of potentially noisy pseudo labels and train a robust student model.</li>
            </ol>
            -->
            
            </div>
            <div class="column">
              <img src="./resources/approach_overview.png" width="80%" />
            </div>
          </div>
        </div>
      </div>
      

      <section class="section">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Model Architecture</h2>
              <div class="content has-text-justified">
                <p class="my-paragraph">
                The initial teacher model (used for pseudo-labeling and filtering) encodes two 
                concatenated image frames and predicts relative camera pose and its uncertainty. 
                To learn effective representations for generalized VO at scale, we propose to 
                incorporate supervision from auxiliary but potentially complementary prediction 
                tasks in addition to the generated VO pseudo-labels.
                Threfore, the complete cross-modal architecture leverages a similar model architecture but with 
                added auxiliary prediction branches with complementary tasks that can further 
                guide self-training, e.g., prediction branches for audio reconstruction, 
                dynamic object segmentation, optical flow, and depth.
                <\p>
              </div>
              <div class="column">
                <img src="./resources/model_structure.png" width="80%" />
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Structure</h2>
          <div class="content has-text-justified">
            
          </div>
            <div class="column">
              <img src="./resources/model_structure.png" width="65%" />
            </div>
          </div>
        </div>
      </div> -->

    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Result</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Analysis on the KITTI Benchmark.</b> 
            We abbreviate 'intrinsics-free' as I (i.e., a method which does not assume the intrinsics) 
            and 'real-world scale' as S (i.e., a method is able to recover real-world scale). To ensure 
            meaningful comparison, we categorize models based on supervision type. Firstly, we present 
            unsupervised learning methods, followed by supervised learning methods, then generalized VO 
            methods, and finally our XVO ablation. In the case of TartanVO, we analyze robustness to 
            noise applied to the intrinsics. We train two teacher models: one based on KITTI (as shown 
            in supervised learning approaches) and the other on nuScenes (as displayed at the end of 
            the Table with ablations).
            </p>
          </div>
          <div class="column">
            <img src="./resources/result1.png" width="90%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Average Quantitative Results across Datasets.</b> 
            We test on KITTI (sequences 00-10), Argoverse 2, and the unseen regions in nuScenes. 
            All results are the average over all scenes. We present translation error, rotation 
            error and scale error. Approaches such as TartanVO do not estimate real-world scale 
            but may be aligned with ground truth (GT) scale in evaluation. A, S, F, D are the 
            abbreviation of Audio, Seg, Flow, Depth.
            </p>
          </div>
          <div class="column">
            <img src="./resources/result2.png" width="80%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Qualitative Analysis on KITTI..</b> 
            We find that incorporating audio and segmentation tasks as part of the semi-supervised learning process significantly improves ego-pose estimation on KITTI.
            </p>
          </div>
          <div class="column">
            <img src="./resources/qua_result.png" width="80%" />
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Qualitative Examples</h2>
              <video controls>
              <source src="./resources/XVO.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section>


  <!-- <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">BibTeX</h2>
          <div class="content has-text-justified">
            <pre><code>@inproceedings{zhang2023coaching,
                                title={Coaching a Teachable Student},
                                author={Zhang, Jimuyang and Huang, Zanming and Ohn-Bar, Eshed},
                                booktitle={CVPR},
                                year={2023}
                        }</code></pre>

          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Acknowledgments</h2>
          <div class="content has-text-centered">
            We thank the Red Hat Collaboratory and National Science Foundation (IIS-2152077) for supporting this research.
          </div>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a href="./resources/XVO.pdf" class="large-font bottom_buttons">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a href="https://github.com/h2xlab/XVO" class="large-font bottom_buttons">
          <i class="fab fa-github"></i>
        </a>
        <br />
        <p>Page template borrowed from <a href="https://https://catdrive.github.io/"><span class="dnerf">CaT</span></a>
      </div>
    </div>
  </footer>

</body>

</html>
