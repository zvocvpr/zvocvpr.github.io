<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="ZeroVO: Visual Odometry with Minimal Assumptions">
  <meta name="keywords" content="ZeroVO: Visual Odometry with Minimal Assumptions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZeroVO: Visual Odometry with Minimal Assumptions</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="shortcut icon" href="./website_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:site_name" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta property="og:description" content="Lai, et al. ZeroVO: Visual Odometry with Minimal Assumptions." />

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="ZeroVO: Visual Odometry with Minimal Assumptions" />
  <meta name="twitter:description" content="Lai, et al. ZeroVO: Visual Odometry with Minimal Assumptions." />

  <script src="https://www.youtube.com/iframe_api"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">ZeroVO: Visual Odometry with Minimal Assumptions</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://leilai125.github.io/">Lei Lai*</a>&emsp;
                <a href="https://zekai-yin.github.io">Zekai Yin*</a>&emsp;
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>&emsp;
                <br />Boston University
                <span class="brmod"></span>CVPR 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="./resources/CAMERA_READY.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.08005" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="#method_video" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/h2xlab/ZVO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/h2xlab/ZVO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Datasets</span>
                  </a>
                </span>
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-6">
        <img src="./resources/example1.gif" />
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
    </div>
  </div>
</section>
 -->
  
  <!-- <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> </h2>
          <div class="content has-text-justified">
            <p>
            </p>
            <div class="column">
              <img src="./resources/generalized_vo.png" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container">

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
                  We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves zero-shot generalization across diverse cameras and environments, overcoming limitations in existing methods that depend on predefined or static camera calibration setups. Our approach incorporates three main innovations. First, we design a calibration-free, geometry-aware network structure capable of handling noise in estimated depth and camera parameters. Second, we introduce a language-based prior that infuses semantic information to enhance robust feature extraction and generalization to previously unseen domains. Third, we develop a flexible, semi-supervised training paradigm that iteratively adapts to new scenes using unlabeled data, further boosting the models' ability to generalize across diverse real-world scenarios. We analyze complex autonomous driving contexts, demonstrating over 30% improvement against prior methods on three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly introduced, high-fidelity synthetic dataset derived from Grand Theft Auto (GTA). By not requiring fine-tuning or camera calibration, our work broadens the applicability of VO, providing a versatile solution for real-world deployment at scale.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
    <!-- <br />
    <br />
    <div id="method_video" class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video"> -->
          <!-- <video controls>
            <source src="./resources/CaT_Video.mp4" type="video/mp4">
          </video> -->
          <!-- <iframe src="https://www.youtube.com/embed/5tmkDHfgqvU"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

  </section>




  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Motivation</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
              Our approach is inspired by how humans learn general representations 
              through observation of large amouts of multi-modal data, 
              and the successes witnessed in computer vision 
              through multi-task frameworks and auxiliary learning.

              <br>
              
              Humans can flexibly estimate motion in arbitrary conditions through 
              a general understanding of salient scene properties (e.g., object sizes).
              This general understanding is developed over large amounts of perceptual 
              data, often multi-modal in nature in arbitrary conditions through a general 
              understanding of salient scene properties.
              For instance, cross-modal information processing between audio and video 
              has been shown to play a role in spatial reasoning and proprioception.
              Drawing from this human learning paradigm, our approach leverages extensive 
              multimodal data through a semi-supervised training process, incorporating 
              various auxiliary prediction tasks, including segmentation, flow, depth, 
              and audio, to achieve generalized visual odometry without any camera intrinsic 
              or extrinsic information in diverse environment under arbitrary conditions.
              Our approach is also motivated by the successes witnessed in computer vision 
              through multi-task frameworks, where auxiliary tasks, in addition to the 
              primary learning objective, serve as a regulation to reduce the overfitting 
              and enrich the feature representations.

              <br>
            </p>
            <div class="column">
              <img src="./resources/motivation.png" />
            </div>
             <p class="my-paragraph">             
              The figure above illustrates the importance of audio in estimating vehicle acceleration and braking.
              The frame is consistent with the red arrow marked on the waveform. On the left, the audio amplitude 
              decreases and maintains a low level when the vehicle is going to wait for traffic lights.
              On the right, the audio experiences many ups and downs associated with acceleration and braking in a narrow urban area.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            Our proposed framework comprises the following steps: 
            (1) uncertainty-aware training of an initial (i.e., teacher) VO model; 
            (2) pseudo-labeling with the removal of low-confidence and potentially noisy samples; 
            (3) self-training with pseudo-labeled and auxiliary prediction tasks of a robust VO student model. 
            Specifically, we first train an ego-motion prediction teacher model over a small initial dataset, e.g., nuScenes.
            We then expand the original dataset through pseudo-labeling of in-the-wild videos.
            After that, we employ multiple auxiliary prediction tasks, including segmentation, flow, depth, and audio, as part of the semi-supervised training process
            Finally, we leverage uncertainty-based filtering of potentially noisy pseudo labels and train a robust student model.
            </p>
            <!--
            <ol>
              <li>We first train an ego-motion prediction teacher model over a small initial dataset, e.g., nuScenes</li>

              <li>We then expand the original dataset through pseudo-labeling of in-the-wild videos.</li>

              <li>we employ multiple auxiliary prediction tasks, including segmentation, flow, depth, and audio, as part of the semi-supervised training process</li>

              <li>We leverage uncertainty-based filtering of potentially noisy pseudo labels and train a robust student model.</li>
            </ol>
            -->
            
            </div>
            <div class="column">
              <img src="./resources/approach_overview.png" width="80%" />
            </div>
          </div>
        </div>
      </div>
      

      <section class="section">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Model Architecture</h2>
              <div class="content has-text-justified">
                <p class="my-paragraph">
                The initial teacher model (used for pseudo-labeling and filtering) encodes two 
                concatenated image frames and predicts relative camera pose and its uncertainty. 
                To learn effective representations for generalized VO at scale, we propose to 
                incorporate supervision from auxiliary but potentially complementary prediction 
                tasks in addition to the generated VO pseudo-labels.
                Threfore, the complete cross-modal architecture leverages a similar model architecture but with 
                added auxiliary prediction branches with complementary tasks that can further 
                guide self-training, e.g., prediction branches for audio reconstruction, 
                dynamic object segmentation, optical flow, and depth.
                <\p>
              </div>
              <div class="column">
                <img src="./resources/model_structure.png" width="80%" />
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Structure</h2>
          <div class="content has-text-justified">
            
          </div>
            <div class="column">
              <img src="./resources/model_structure.png" width="65%" />
            </div>
          </div>
        </div>
      </div> -->

    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Result</h2>
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Analysis on the KITTI Benchmark.</b> 
            We abbreviate 'intrinsics-free' as I (i.e., a method which does not assume the intrinsics) 
            and 'real-world scale' as S (i.e., a method is able to recover real-world scale). To ensure 
            meaningful comparison, we categorize models based on supervision type. Firstly, we present 
            unsupervised learning methods, followed by supervised learning methods, then generalized VO 
            methods, and finally our XVO ablation. In the case of TartanVO, we analyze robustness to 
            noise applied to the intrinsics. We train two teacher models: one based on KITTI (as shown 
            in supervised learning approaches) and the other on nuScenes (as displayed at the end of 
            the Table with ablations).
            </p>
          </div>
          <div class="column">
            <img src="./resources/result1.png" width="90%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Average Quantitative Results across Datasets.</b> 
            We test on KITTI (sequences 00-10), Argoverse 2, and the unseen regions in nuScenes. 
            All results are the average over all scenes. We present translation error, rotation 
            error and scale error. Approaches such as TartanVO do not estimate real-world scale 
            but may be aligned with ground truth (GT) scale in evaluation. A, S, F, D are the 
            abbreviation of Audio, Seg, Flow, Depth.
            </p>
          </div>
          <div class="column">
            <img src="./resources/result2.png" width="80%" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p class="my-paragraph">
            <b>Qualitative Analysis on KITTI..</b> 
            We find that incorporating audio and segmentation tasks as part of the semi-supervised learning process significantly improves ego-pose estimation on KITTI.
            </p>
          </div>
          <div class="column">
            <img src="./resources/qua_result.png" width="80%" />
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Qualitative Examples</h2>
              <video controls>
              <source src="./resources/examples.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section>


  <!-- <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">BibTeX</h2>
          <div class="content has-text-justified">
            <pre><code>@inproceedings{zhang2023coaching,
                                title={Coaching a Teachable Student},
                                author={Zhang, Jimuyang and Huang, Zanming and Ohn-Bar, Eshed},
                                booktitle={CVPR},
                                year={2023}
                        }</code></pre>

          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section id=" BibTeX"">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Acknowledgments</h2>
          <div class="content has-text-centered">
            We thank the Red Hat Collaboratory and National Science Foundation (IIS-2152077) for supporting this research.
          </div>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a href="./resources/CAMERA_READY.pdf" class="large-font bottom_buttons">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a href="https://github.com/h2xlab/ZVO" class="large-font bottom_buttons">
          <i class="fab fa-github"></i>
        </a>
        <br />
        <p>Page template borrowed from <a href="https://https://catdrive.github.io/"><span class="dnerf">CaT</span></a>
      </div>
    </div>
  </footer>

</body>

</html>
